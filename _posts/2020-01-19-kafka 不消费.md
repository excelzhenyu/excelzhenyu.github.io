---
layout: post
title: spring kafka 不消费
category : kafka
tagline: "Supporting tagline"
tags : [kafka]
---
{% include JB/setup %}
# spring kafka 不消费
---

使用spring 的@KafkaListener 来处理kafka消息，两个系统使用kafka通信解耦，最开始是没有问题的，后来测试发现，A系统发出消息之后，B系统很久没有响应，开始了如下的解决思路
0. 定位问题
首先查看日志，因为搭建了elk,直接在kibana搜索对应的log,发现B系统的kafkalistener仍然在处理消息，虽然日志有报错，但属于业务原因，正常。排除系统故障的原因；
然后使用kafka命令查看当前是否有消息积压，如果有消息积压了，那可能这个没有响应是因为还没有处理到，查看积压的命令如下：

```
./kafka-consumer-groups.sh --bootstrap-server {master:9092} --describe --group  {group_id}
```
查到如下信息：
TOPIC                        PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                      HOST            CLIENT-ID
topic_name                   0          50              492             442               consumer-25-96cb8655-d5ba-4313-9dc7-9907dfbb7831 /ip             consumer-25

其中current-offset为当前处理到的位置，log-end-offset为当前最大位置，lag = （log-end-offset）- （current-offset)，为积压条数，可以看到积压条数为442
是有积压，但是才400多条，应该很快就处理完了，于是等了几分钟，再次查看积压条数，发现还是这么多，很明显这里有问题了，开始排查代码，发现代码中有抛异常的部分，
而当前处理的消息是一条脏数据，是必然抛异常的，kafkalistener在处理的时候发现处理失败，于是就没有更新offset，下次还是处理这条记录，陷入循环，，，
知道了原因，接下来就好办了，在处理kafka消息的最外层加一个try catch, 使异常不被抛出，offset能被顺利更新，重新部署，问题解决。



